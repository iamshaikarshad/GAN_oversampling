# GAN-Based Oversampling for Customer Retention Prediction

This project implements a Generative Adversarial Network (GAN) to address class imbalance in customer retention datasets.
By generating synthetic samples for the minority class, the model aims to improve the performance of classification algorithms in predicting customer retention.

## Overview

Class imbalance is a common issue in predictive modeling, where the number of instances in one class significantly outnumbers the other.
Traditional oversampling techniques like SMOTE or random oversampling may not capture the underlying data distribution effectively.
This project leverages GANs to generate realistic synthetic data for the minority class, thereby enhancing the classifier's ability to learn from balanced data.

## Understanding Generative Adversarial Networks (GANs)

A Generative Adversarial Network (GAN) is a fascinating deep learning model architecture that involves two neural networks, a Generator and a Discriminator, competing against each other in a zero-sum game. This adversarial process allows the Generator to learn how to create new data that is indistinguishable from real data.
The Two Players:

1. **The Generator (The Artist):**

   - Takes random noise as input.

   - Its goal is to learn to produce data (e.g., images, text, audio) that looks authentic enough to fool the Discriminator.

   - It's like an artist trying to create convincing forgeries.

2. **The Discriminator (The Critic/Detector):**

   - Receives two types of input: real data from the training dataset and fake data generated by the Generator.

   - Its goal is to distinguish between real and fake data.

   - It's like an art critic trying to spot a forgery.

## How They Train (The Adversarial Game):

The training process is a continuous cycle of competition and improvement:
Step 1: Discriminator Training (Real Data)

The Discriminator is first shown real data and learns to correctly classify it as "real."
```
[Real Data]  ----->  [Discriminator]  ----->  "Real" (Correct!)
```
Step 2: Discriminator Training (Fake Data)

The Generator produces fake data from random noise. This fake data is then fed to the Discriminator, which tries to classify it as "fake."
```
[Random Noise]  ----->  [Generator]  ----->  [Fake Data]  ----->  [Discriminator]  ----->  "Fake" (Correct!)
```
Step 3: Generator Training (Feedback)

The Generator's goal is to fool the Discriminator. When the Discriminator correctly identifies a fake image, the Generator receives feedback (gradients) indicating how its generated data was flawed. The Generator then adjusts its parameters to produce more convincing fakes.
```
[Discriminator]  ----->  [Feedback (e.g., "Too blurry")]  ----->  [Generator]
```
Step 4: Iteration and Improvement

This process repeats over many iterations:

   - The Generator gets better at creating realistic data.

   - The Discriminator gets better at detecting fakes.

As the Generator improves, the Discriminator finds it harder and harder to distinguish between real and fake data.

Loop:
   - Generator creates better fakes
   - Discriminator tries to catch them
   - Both learn and improve

Step 5: Convergence (The Goal)

Eventually, the Generator becomes so good that the Discriminator can no longer reliably tell the difference between real and generated data. At this point, the Discriminator will output "Real" (or a probability close to 0.5) for both real and fake inputs.
```
[Random Noise]  ----->  [Generator]  ----->  [Highly Realistic Fake Data]  ----->  [Discriminator]  ----->  "Real" (Fooled!)
```
The Generator has successfully learned to produce new, high-quality data that resembles the original training data distribution.

## Project Structure

- **`retention_Oversampling_Gan.ipynb`**: Jupyter Notebook containing the implementation of the GAN model for oversampling.
- **`data/`**: Directory containing the dataset used for training and evaluation.
- **`models/`**: Directory to save trained models and generated synthetic data.
- **`README.md`**: Project documentation.

## Conceptual Diagram

Below is a conceptual diagram illustrating the GAN-based oversampling process:

```
[Minority Class Data] --> [Generator] --> [Synthetic Data]
                                     |
                                     v
[Real Data] --> [Discriminator] <--> [Synthetic Data]
                                     |
                                     v
                            [Improved Classifier]
```

In this setup:
- The **Generator** creates synthetic data resembling the minority class.
- The **Discriminator** evaluates the authenticity of the synthetic data against real data.
- The **Classifier** is trained on the augmented dataset, leading to improved performance.

## Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/iamshaikarshad/GAN_oversampling.git
   cd GAN_oversampling
   ```

2. **Create a virtual environment (optional but recommended):**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install the required packages:**
   ```bash
   pip install -r requirements.txt
   ```

## Usage

1. **Prepare the dataset:**
   - Place your dataset in the `data/` directory. Ensure it is in CSV format with appropriate preprocessing.

2. **Run the Jupyter Notebook:**
   ```bash
   jupyter notebook retention_Oversampling_Gan.ipynb
   ```

3. **Follow the steps in the notebook:**
   - Load and preprocess the data.
   - Train the GAN model to generate synthetic minority class samples.
   - Augment the original dataset with synthetic samples.
   - Train and evaluate the classifier on the balanced dataset.

## Results

The GAN-based oversampling approach demonstrated improved classification metrics compared to traditional oversampling methods.
By generating realistic synthetic data, the classifier achieved higher accuracy and better generalization on unseen data.

### Distribution Comparisons

**Feature 1 Distribution**  
![image alt](https://github.com/iamshaikarshad/GAN_oversampling/blob/0c0f331f41c0ec76e71091a053ebb21009753523/src/images/savings%20Orignal%20vs%20generated.png)

**Feature 2 Distribution**  
![image alt](https://github.com/iamshaikarshad/GAN_oversampling/blob/0c0f331f41c0ec76e71091a053ebb21009753523/src/images/Total_Cost.png)

**Feature 3 Distribution**  
![image alt](https://github.com/iamshaikarshad/GAN_oversampling/blob/9073139161e1bf440a185505ebbdc1258f354a7f/src/images/Column.png?raw=true)

These charts compare the distributions of original and generated data across key features. Note the close similarity in shape and statistical metrics such as mean and variance.

## References

- Chawla, N. V., et al. (2002). "SMOTE: Synthetic Minority Over-sampling Technique." Journal of Artificial Intelligence Research, 16, 321â€“357.
- Goodfellow, I., et al. (2014). "Generative Adversarial Nets." Advances in Neural Information Processing Systems, 27.

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
